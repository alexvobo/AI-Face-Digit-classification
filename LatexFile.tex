\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{tikz}
\usepackage{graphicx}

\title{Image Classification}
\author{Jaehyun Kim, Alex Vobornov}
\date{August 2019}

\begin{document}

\maketitle

\section{Perceptron}
    \subsection{Extracting}
        \hspace*{10mm}In classifier.py, the raw data from training data returns a set of pixel features indicating whether each pixel in the provided datum is '0' if it is empty ,or '1' if it is '+' or '\#'. This occurs in basicFeatureExtractorDigit and basicFeatureExtractorFace functions. For example, the data will be $\{(0,0):0,(0,1):0,(0,2):1...(28,28):.0\}.$ 
   
    \subsection{Perceptron Classifier}
        \hspace*{10mm}The Perceptron classifier is called by classifier.py. Arguments of 'legalLabels' and 'option.iterations' are used. ‘LegalLabels’ is a list that uses 0 to 9 to compare labels for digits, and 0 to 1 for faces. ‘option.iterations’ is the option with -i that indicates how many to repeat the training. The default iteration is 3. That means hidden layers are 3. \newline
        For each layer, we scan one instance at a time and find the label with the highest score. To get a high score, we set y as real label and y' is guess label. The guess label can get from this formula \ y' = arg max score(feature, y")\ . \newline
        To get y', the guessed label, we use classify function in perceptron.py. We Multiply weight[label] and current data, datum. This means multiplying each features of value and then return the sum of the values to vector which is type of list. 
        For example,  
        \begin{quote}
            vectors[l] = weights[l] * datum\\
            = $sum += weight[label] * datum[label]$\\
            = $sum += (a,b):z * (a',b'):z' = z * z'$\\
            = $((0,0):0 * (0',0'):0)+...+((28,28):0 * (28,28):0)$\\
            vector[l] = (0*0)+...+(0*0)\\
            vector = \{0:0, 1:144, 2:4, ... , 9:0\}
        \end{quote}
        After calculating all of the vectors, we choose the maximum number in the vector list and using argMax function, which is in util.py, we return the highest key of the highest value.
        
        And then, We compare y and y'. If y and y' are same then skip because we guessed correctly. If y and y' are not matching. It means that we guessed y' but we should have guessed y. The weight of y should have scored f higher and weight of y' should have scored f lower. \newline
        In our code, for example, we used training data to see how y and y' work. \newline
        \begin{quote}
        ('Starting iteration ', 0, '...')\\
        We have guessed 0 but should have guessed 5\\
        We have guessed 5 but should have guessed 0\\
        We have guessed 0 but should have guessed 4\\
        We have guessed 5 but should have guessed 1\\
        We have guessed 0 but should have guessed 9\\
        We have guessed 0 but should have guessed 2\\
        We have guessed 9 but should have guessed 1\\
        We have guessed 1 but should have guessed 3\\
        We have guessed 1 but should have guessed 4\\
        We have guessed 1 but should have guessed 3\\
        We have guessed 3 but should have guessed 5\\
        We have guessed 3 but should have guessed 6\\
        We have guessed 3 but should have guessed 1\\
        We have guessed 3 but should have guessed 7\\
        We have guessed 3 but should have guessed 2\\
        We have guessed 3 but should have guessed 8\\
        We have guessed 3 but should have guessed 6\\
        We have guessed 1 but should have guessed 9\\
        We have guessed 3 but should have guessed 4\\
        We have guessed 3 but should have guessed 0\\
        We have guessed 4 but should have guessed 9\\
        We have guessed 9 but should have guessed 1\\
        We have guessed 3 but should have guessed 2\\
        We have guessed 2 but should have guessed 4\\
        We have guessed 2 but should have guessed 3\\
        We have guessed 3 but should have guessed 2\\
        We have guessed 2 but should have guessed 7\\
        We have guessed 2 but should have guessed 3\\
        We have guessed 3 but should have guessed 8\\
        We have guessed 3 but should have guessed 6\\
        We have guessed 3 but should have guessed 9\\
        We have guessed 3 but should have guessed 0\\
        We have guessed 3 but should have guessed 5\\
        We have guessed 3 but should have guessed 6\\
        We have guessed 2 but should have guessed 0\\
        We have guessed 2 but should have guessed 7\\
        We have guessed 3 but should have guessed 1\\
        We have guessed 0 but should have guessed 8\\
        We have guessed 3 but should have guessed 7\\
        We have guessed 0 but should have guessed 9\\
        We have guessed 3 but should have guessed 8\\
        We have guessed 0 but should have guessed 5\\
        We have guessed 3 but should have guessed 9\\
        We have guessed 0 but should have guessed 7\\
        We have guessed 9 but should have guessed 4\\
        We have guessed 9 but should have guessed 8\\
        We have guessed 9 but should have guessed 4\\
        We have guessed 8 but should have guessed 1\\
        We have guessed 8 but should have guessed 4\\
        We have guessed 4 but should have guessed 6\\
        We have guessed 6 but should have guessed 4\\
        We have guessed 4 but should have guessed 5\\
        We have guessed 8 but should have guessed 1\\
        We have guessed 2 but should have guessed 0\\
        We have guessed 4 but should have guessed 0\\
        We have guessed 0 but should have guessed 1\\
        We have guessed 4 but should have guessed 7\\
        We have guessed 4 but should have guessed 6\\
        We have guessed 0 but should have guessed 3\\
        We have guessed 1 but should have guessed 2\\
        We have guessed 4 but should have guessed 7\\
        We have guessed 0 but should have guessed 9\\
        We have guessed 0 but should have guessed 2\\
        We have guessed 2 but should have guessed 7\\
        We have guessed 7 but should have guessed 9\\
        We have guessed 9 but should have guessed 4\\
        We have guessed 9 but should have guessed 8\\
        We have guessed 9 but should have guessed 7\\
        ('Starting iteration ', 1, '...')\\
        We have guessed 8 but should have guessed 5\\
        We have guessed 4 but should have guessed 3\\
        We have guessed 4 but should have guessed 5\\
        We have guessed 8 but should have guessed 1\\
        We have guessed 7 but should have guessed 1\\
        We have guessed 7 but should have guessed 4\\
        We have guessed 1 but should have guessed 8\\
        We have guessed 7 but should have guessed 9\\
        We have guessed 8 but should have guessed 0\\
        We have guessed 1 but should have guessed 5\\
        We have guessed 0 but should have guessed 6\\
        We have guessed 8 but should have guessed 5\\
        We have guessed 0 but should have guessed 3\\
        We have guessed 9 but should have guessed 4\\
        We have guessed 4 but should have guessed 9\\
        We have guessed 9 but should have guessed 8\\
        We have guessed 8 but should have guessed 1\\
        We have guessed 9 but should have guessed 7\\
        We have guessed 1 but should have guessed 2\\
        We have guessed 0 but should have guessed 9\\
        We have guessed 9 but should have guessed 6\\
        We have guessed 9 but should have guessed 3\\
        We have guessed 9 but should have guessed 7\\
        ('Starting iteration ', 2, '...')\\
        We have guessed 3 but should have guessed 5\\
        We have guessed 9 but should have guessed 2\\
        We have guessed 9 but should have guessed 1\\
        We have guessed 2 but should have guessed 0\\
        \end{quote}
            
        In this case, we updated the weights as \ $w^y = w^y + f$ \ and \ $w^y' = w^y' - f$.\ In the code, we uses util.py methods which are $\_\_$radd$\_\_$ and $\_\_$sub$\_\_$ to increment/decrement counters.
        \begin{quote}
            $self.weights[y].$\_\_$radd$\_\_$(trainingData[i])$\\
            $self.weights[yPrime].$\_\_$sub$\_\_$(trainingData[i])$
        \end{quote}
            
    \subsection{Training data \& Analysis}
        \subsubsection{Digit}
            \begin{table}[h]
                \centering
                \begin{tabular}{c|c|c|c}
                    \hline
                        Training Data Used & Labels out of 5000 & Validation Accuracy & Test Accuracy \\
                    \hline
                        10\% & 500 & 81\% & 76\%\\
                    \hline
                        20\% & 1000 & 77\% & 79\%\\
                    \hline
                        30\% & 1500 & 80\% & 78\%\\
                    \hline
                        40\% & 2000 & 85\% & 77\%\\
                    \hline
                        50\% & 2500 & 84\% & 82\%\\
                    \hline
                        60\% & 3000 & 80\% & 82\%\\
                    \hline
                        70\% & 3500 & 81\% & 84\%\\
                    \hline
                        80\% & 4000 & 83\% & 86\%\\
                    \hline
                        90\% & 4500 & 84\% & 87\%\\
                    \hline
                        100\% & 5000 & 82\% & 86\%\\
                    \hline
                \end{tabular}
                \caption{Percentage of training data with iteration 3}
            \end{table}
        \newpage
        \subsubsection{Face}
            \begin{table}[h]
                \centering
                \begin{tabular}{c|c|c|c}
                    \hline
                        Training Data Used & Labels out of 450 & Validation Accuracy & Test Accuracy \\
                    \hline
                        10\% & 45 & 74\% & 62\%\\
                    \hline
                        20\% & 90 & 91\% & 76\%\\
                    \hline
                        30\% & 135 & 91\% & 72\%\\
                    \hline
                        40\% & 180 & 94\% & 81\%\\
                    \hline
                        50\% & 225 & 95\% & 79\%\\
                    \hline
                        60\% & 270 & 99\% & 87\%\\
                    \hline
                        70\% & 315 & 95\% & 80\%\\
                    \hline
                        80\% & 360 & 93\% & 80\%\\
                    \hline
                        90\% & 405 & 98\% & 82\%\\
                    \hline
                        100\% & 450 & 100\% & 85\%\\
                    \hline
                \end{tabular}
                \caption{Percentage of face training data with iteration 3}
            \end{table}
         
\section{Naive Bayes}
    \subsection{Prior Probability in Naive classifier}
        Our naive Bayes model has several parameters to estimate. One parameter is the prior probability over labels (digits, or face/not-face), $\Pr[Y]$.\\
        To estimate $\Pr[Y]$, we extract data from training data labels. 
        \begin{displaymath}
        \Pr[y] = \frac{c(y)}{n}
        \end{displaymath}
        To implemet this formula, we use incrementAll, and normalize function from Counter() class in util.py. First initialize dictionary using Counter class and using incrementAll function, all the label keys to set 0. And then, count all of the label in the training data and save to dictionary, which is total\_cnt\_label in our code. For example, using 100 of labels, it looks \{0: 13, 1: 14, 2: 6, 3: 11, 4: 11, 5: 5, 6: 11, 7: 10, 8: 8, 9: 11\}. Next step is that using normalize function, we divide each value by the sum of all values. For example, it looks \{0: 0.13, 1: 0.14, 2: 0.06, 3: 0.11, 4: 0.11, 5: 0.05, 6: 0.11, 7: 0.1, 8: 0.08, 9: 0.11\}.
        Final step is that declare self.prior\_probabilty variable and put it in this value so that we can use the other function to calculate base on Naive Bayes. 

    \subsection{Conditional Probabilities in Naive classifier}
        The next estimation is the conditional probabilities of our features given each label y: $\Pr[F_i \vert Y = y]$.
        \begin{eqnarray*}
        \Pr[F_i=f_i\vert Y=y] &=& \frac{c(f_i,y)}{\sum_{f'_i\in \{0,1\}}{c(f_i,y)}} \\
        \end{eqnarray*}
        First, getting a cost of feature and label set, we extract data from training data. While extracting data, to calculate the formula in the denominator (${\sum_{f'_i\in \{0,1\}}{c(f_i,y)}}$), we declare conditional\_feature\_label with util.Counter() for counting value 1 which means that during searching pixels and if pixel is 1 then increment 1. We also declare non\_conditional\_feature\_label for counting value 0. For example, the format of these two variables look like \{(feature, label):value ... \} and this is for real example from our code $\{((4, 23), 5): 1, ((4, 24), 5): 1, ((5, 23), 5): 1,...\}$. Following the formula which denominator, we declare total\_features\_labels and add conditional\_feature\_label with non\_conditional\_feature\_label.
        
        Second, to calculate the formula in the numerator ($c(f_i,y)$), we used already declared variable which is conditional\_feature\_label with util.Counter(). In this case, we already added all of information to the variable. \\
        
        Third step is that we need to smooth our cost of feature and label sets. It helps to improve more accuracy and reduce overfitting to recognize digit and face. In our project, we fixed smooth value as 0.05 instead default value because 0.05 show us the highest accuracy and reduce running time.\\
        
        Final step is to get conditional probability. Following the formula, we already have all information to get conditional probability, so we extract value from conditional\_feature\_label and total\_features\_labels. And then divide using that two information and save to variable. To get a posterior probability, we called classify function. This function is also called calculateLogJointProbabilities fucntion to calculate log joint probability. As Berkeley project mention, during using Bayes theorem, multiplying many probabilities together often results in underflow, so instead we use log probability. We use characteristic of log that logMN = logM + logN which means log(prior) + log(conditional) which is likelihood.
        \begin{eqnarray*}
        \textmd{arg max}_{y} log(P(y \vert f_1, \ldots, f_m) &=& {arg max}_{y} (log(P(y)) + \sum_{i = 1}^m log(P(f_i \vert y)))
        \end{eqnarray*}
        For implementing this code, we encountered an issue that 'ValueError: math domain error' was occurred. Ater debugging the code, we found the solution that log must not include less than or equal to 0, so we check if value is less than 0 then we just set 1 to avoid error because log 1 is 0.
        **explain here why log used. ex. logMN = logM + logN. this is why we add in calsthsthsth function

    \subsection{Training data \& Analysis}
        \subsubsection{digit}
            \begin{table}[h]
                \centering
                \begin{tabular}{c|c|c|c}
                    \hline
                        Training Data Used & Labels out of 5000 & Validation Accuracy & Test Accuracy \\
                    \hline
                        10\% & 500 & 83\% & 74\%\\
                    \hline
                        20\% & 1000 & 80\% & 78\%\\
                    \hline
                        30\% & 1500 & 82\% & 79\%\\
                    \hline
                        40\% & 2000 & 85\% & 78\%\\
                    \hline
                        50\% & 2500 & 84\% & 80\%\\
                    \hline
                        60\% & 3000 & 83\% & 82\%\\
                    \hline
                        70\% & 3500 & 84\% & 81\%\\
                    \hline
                        80\% & 4000 & 82\% & 81\%\\
                    \hline
                        90\% & 4500 & 83\% & 79\%\\
                    \hline
                        100\% & 5000 & 84\% & 79\%\\
                    \hline
                \end{tabular}
                \caption{Percentage of digit training data}
            \end{table}
        \newpage
        \subsubsection{Face}
            \begin{table}[h]
                \centering
                \begin{tabular}{c|c|c|c}
                    \hline
                        Training Data Used & Labels out of 450 & Validation Accuracy & Test Accuracy \\
                    \hline
                        10\% & 45 & 68\% & 53\%\\
                    \hline
                        20\% & 90 & 95\% & 82\%\\
                    \hline
                        30\% & 135 & 98\% & 85\%\\
                    \hline
                        40\% & 180 & 98\% & 85\%\\
                    \hline
                        50\% & 225 & 97\% & 83\%\\
                    \hline
                        60\% & 270 & 97\% & 85\%\\
                    \hline
                        70\% & 315 & 97\% & 86\%\\
                    \hline
                        80\% & 360 & 97\% & 86\%\\
                    \hline
                        90\% & 405 & 95\% & 88\%\\
                    \hline
                        100\% & 450 & 95\% & 88\%\\
                    \hline
                \end{tabular}
                \caption{Percentage of face training data with iteration 3}
            \end{table}
\section{Conclusion}
\hspace*{10mm}In the conclusion, the different thing between Perceptron and Naive Bayes is that Perceptron uses weight instead of probability to learning data. On the other hands, Naive Bayes is based on Bayes rule, so it depends on probability to learning data.\\
Both Perceptron and Naive Bayes classifier show similar accuracy. However Naive Bayes accuracy is a little bit higher than Perceptron that include face and digit. The result graphs show in 3.1 Comparing data \& Analysis.


    \newpage
    \subsection{Comparing data \& Analysis }
\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{0809054340386020.jpg}
    \caption{Comparing between Perceptron and Navie Bayes}
\end{figure}

\end{document}
