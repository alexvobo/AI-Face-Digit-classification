\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{tikz}
\usepackage{graphicx}

\title{Image Classification}
\author{Jaehyun Kim, Alex Vobornov}
\date{August 2019}

\begin{document}

\maketitle

\section{Perceptron}
    \subsection{Extracting}
        \hspace*{10mm}In the classifier.py, the raw data from training data return a set of pixel features indicating whether each pixel in the provided datum is '0' if it is empty ,or '1' if it is '+' or '\#'. This works occurs at basicFeatureExtractorDigit and basicFeatureExtractorFace functions. For example, the data will be \{(0,0):0,(0,1):0,(0,2):1...(28,28):.0\}. 
   
    \subsection{Perceptron Classifier}
        \hspace*{10mm}PerceptronClassifier is called by classifier. Arguments of 'legalLabels' and 'option.iterations' are used. legalLabels is type of list that include 0 to 9 to compare labels. option.iterations is the option with -i that how many repeat the training. The default iteration is 3. That means hidden layers are 3. \newline
        For each layer, we scan one instance at a time and find the label with the highest score. To get a high score, we set y as real label and y' is guess label. The guess label can get from this formula \ y' = arg max score(feature, y")\ . \newline
        To get y', guessed label, we use classify function in perceptron class. Multiply weight[label] and current data as datum. It means multiply each features of value and then return the sum of the values to vector which is type of list. 
        For example,  
        \begin{quote}
            vectors[l] = weights[l] * datum\\
            = $sum += weight[label] * datum[label]$\\
            = $sum += (a,b):z * (a',b'):z' = z * z'$\\
            = $((0,0):0 * (0',0'):0)+...+((28,28):0 * (28,28):0)$\\
            vector[l] = (0*0)+...+(0*0)\\
            vector = \{0:0, 1:144, 2:4, ... , 9:0\}
        \end{quote}
        After calculating all of the vectors, we choose the maximum number in the vector list and using argMax function, which is in util.py, we return the highest key of the highest value.
        
        And then, We compare y and y'. If y and y' are same then skip. If y and y' are not matching. It means that we guessed y' but we should have guessed y. The weight of y should have scored f higher and weight of y' should have scored f lower. \newline
        In our code, for example, we used 100 of training data to see how y and y' work. \newline
        \begin{quote}
        ('Starting iteration ', 0, '...')\\
        We have guessed 0 but should have guessed 5\\
        We have guessed 5 but should have guessed 0\\
        We have guessed 0 but should have guessed 4\\
        We have guessed 5 but should have guessed 1\\
        We have guessed 0 but should have guessed 9\\
        We have guessed 0 but should have guessed 2\\
        We have guessed 9 but should have guessed 1\\
        We have guessed 1 but should have guessed 3\\
        We have guessed 1 but should have guessed 4\\
        We have guessed 1 but should have guessed 3\\
        We have guessed 3 but should have guessed 5\\
        We have guessed 3 but should have guessed 6\\
        We have guessed 3 but should have guessed 1\\
        We have guessed 3 but should have guessed 7\\
        We have guessed 3 but should have guessed 2\\
        We have guessed 3 but should have guessed 8\\
        We have guessed 3 but should have guessed 6\\
        We have guessed 1 but should have guessed 9\\
        We have guessed 3 but should have guessed 4\\
        We have guessed 3 but should have guessed 0\\
        We have guessed 4 but should have guessed 9\\
        We have guessed 9 but should have guessed 1\\
        We have guessed 3 but should have guessed 2\\
        We have guessed 2 but should have guessed 4\\
        We have guessed 2 but should have guessed 3\\
        We have guessed 3 but should have guessed 2\\
        We have guessed 2 but should have guessed 7\\
        We have guessed 2 but should have guessed 3\\
        We have guessed 3 but should have guessed 8\\
        We have guessed 3 but should have guessed 6\\
        We have guessed 3 but should have guessed 9\\
        We have guessed 3 but should have guessed 0\\
        We have guessed 3 but should have guessed 5\\
        We have guessed 3 but should have guessed 6\\
        We have guessed 2 but should have guessed 0\\
        We have guessed 2 but should have guessed 7\\
        We have guessed 3 but should have guessed 1\\
        We have guessed 0 but should have guessed 8\\
        We have guessed 3 but should have guessed 7\\
        We have guessed 0 but should have guessed 9\\
        We have guessed 3 but should have guessed 8\\
        We have guessed 0 but should have guessed 5\\
        We have guessed 3 but should have guessed 9\\
        We have guessed 0 but should have guessed 7\\
        We have guessed 9 but should have guessed 4\\
        We have guessed 9 but should have guessed 8\\
        We have guessed 9 but should have guessed 4\\
        We have guessed 8 but should have guessed 1\\
        We have guessed 8 but should have guessed 4\\
        We have guessed 4 but should have guessed 6\\
        We have guessed 6 but should have guessed 4\\
        We have guessed 4 but should have guessed 5\\
        We have guessed 8 but should have guessed 1\\
        We have guessed 2 but should have guessed 0\\
        We have guessed 4 but should have guessed 0\\
        We have guessed 0 but should have guessed 1\\
        We have guessed 4 but should have guessed 7\\
        We have guessed 4 but should have guessed 6\\
        We have guessed 0 but should have guessed 3\\
        We have guessed 1 but should have guessed 2\\
        We have guessed 4 but should have guessed 7\\
        We have guessed 0 but should have guessed 9\\
        We have guessed 0 but should have guessed 2\\
        We have guessed 2 but should have guessed 7\\
        We have guessed 7 but should have guessed 9\\
        We have guessed 9 but should have guessed 4\\
        We have guessed 9 but should have guessed 8\\
        We have guessed 9 but should have guessed 7\\
        ('Starting iteration ', 1, '...')\\
        We have guessed 8 but should have guessed 5\\
        We have guessed 4 but should have guessed 3\\
        We have guessed 4 but should have guessed 5\\
        We have guessed 8 but should have guessed 1\\
        We have guessed 7 but should have guessed 1\\
        We have guessed 7 but should have guessed 4\\
        We have guessed 1 but should have guessed 8\\
        We have guessed 7 but should have guessed 9\\
        We have guessed 8 but should have guessed 0\\
        We have guessed 1 but should have guessed 5\\
        We have guessed 0 but should have guessed 6\\
        We have guessed 8 but should have guessed 5\\
        We have guessed 0 but should have guessed 3\\
        We have guessed 9 but should have guessed 4\\
        We have guessed 4 but should have guessed 9\\
        We have guessed 9 but should have guessed 8\\
        We have guessed 8 but should have guessed 1\\
        We have guessed 9 but should have guessed 7\\
        We have guessed 1 but should have guessed 2\\
        We have guessed 0 but should have guessed 9\\
        We have guessed 9 but should have guessed 6\\
        We have guessed 9 but should have guessed 3\\
        We have guessed 9 but should have guessed 7\\
        ('Starting iteration ', 2, '...')\\
        We have guessed 3 but should have guessed 5\\
        We have guessed 9 but should have guessed 2\\
        We have guessed 9 but should have guessed 1\\
        We have guessed 2 but should have guessed 0\\
        \end{quote}
            
        In this case, we updated the weights as \ $w^y = w^y + f$ \ and \ $w^y' = w^y' - f$.\ In the code, we uses util.py methods which are $\_\_$radd$\_\_$ and $\_\_$sub$\_\_$ to increment/decrement counters.
        \begin{quote}
            $self.weights[y].$\_\_$radd$\_\_$(trainingData[i])$\\
            $self.weights[yPrime].$\_\_$sub$\_\_$(trainingData[i])$
        \end{quote}
            
    \subsection{Training data \& Analysis}
        \subsubsection{Digit}
            \begin{table}[h]
                \centering
                \begin{tabular}{c|c|c|c}
                    \hline
                        Training Data Used & Labels out of 5000 & Validation Accuracy & Test Accuracy \\
                    \hline
                        10\% & 500 & 81\% & 76\%\\
                    \hline
                        20\% & 1000 & 77\% & 79\%\\
                    \hline
                        30\% & 1500 & 80\% & 78\%\\
                    \hline
                        40\% & 2000 & 85\% & 77\%\\
                    \hline
                        50\% & 2500 & 84\% & 82\%\\
                    \hline
                        60\% & 3000 & 80\% & 82\%\\
                    \hline
                        70\% & 3500 & 81\% & 84\%\\
                    \hline
                        80\% & 4000 & 83\% & 86\%\\
                    \hline
                        90\% & 4500 & 84\% & 87\%\\
                    \hline
                        100\% & 5000 & 82\% & 86\%\\
                    \hline
                \end{tabular}
                \caption{Percentage of training data with iteration 3}
            \end{table}
            \newpage
            \begin{figure}[h]
                \centering
                    \includegraphics[width=0.3\linewidth]{digitdata.PNG}
                \caption{Digit training data}
            \end{figure}
        
        \subsubsection{Face}
            \begin{table}[h]
                \centering
                \begin{tabular}{c|c|c|c}
                    \hline
                        Training Data Used & Labels out of 450 & Validation Accuracy & Test Accuracy \\
                    \hline
                        10\% & 45 & 74\% & 62\%\\
                    \hline
                        20\% & 90 & 91\% & 76\%\\
                    \hline
                        30\% & 135 & 91\% & 72\%\\
                    \hline
                        40\% & 180 & 94\% & 81\%\\
                    \hline
                        50\% & 225 & 95\% & 79\%\\
                    \hline
                        60\% & 270 & 99\% & 87\%\\
                    \hline
                        70\% & 315 & 95\% & 80\%\\
                    \hline
                        80\% & 360 & 93\% & 80\%\\
                    \hline
                        90\% & 405 & 98\% & 82\%\\
                    \hline
                        100\% & 450 & 100\% & 85\%\\
                    \hline
                \end{tabular}
                \caption{Percentage of face training data with iteration 3}
            \end{table}
            
            \begin{figure}[h]
                \centering
                \includegraphics[width=0.3\linewidth]{face.png}
                \caption{Face training data}
            \end{figure}
        \newpage

\section{Naive Bayes}
    \subsection{Prior Probability}
        Our naive Bayes model has several parameters to estimate. One parameter is the prior probability over labels (digits, or face/not-face), $\Pr[Y]$.\\
        To estimate $\Pr[Y]$, we extract data from training data labels. 
        \begin{displaymath}
        \Pr[y] = \frac{c(y)}{n}
        \end{displaymath}
        To implemet this formula, we use incrementAll, and normalize function from Counter() class in util.py. First initialize dictionary using Counter class and using incrementAll function, all the label keys to set 0. And then, count all of the label in the training data and save to dictionary, which is total\_cnt\_label in our code. For example, using 100 of labels, it looks \{0: 13, 1: 14, 2: 6, 3: 11, 4: 11, 5: 5, 6: 11, 7: 10, 8: 8, 9: 11\}. Next step is that using normalize function, we divide each value by the sum of all values. For example, it looks \{0: 0.13, 1: 0.14, 2: 0.06, 3: 0.11, 4: 0.11, 5: 0.05, 6: 0.11, 7: 0.1, 8: 0.08, 9: 0.11\}.
        Final step is that declare self.prior\_probabilty variable and put it in this value so that we can use the other function to calculate base on Naive Bayes. 

    \subsection{Conditional Probabilities}
        The next estimation is the conditional probabilities of our features given each label y: $\Pr[F_i \vert Y = y]$.
        \begin{eqnarray*}
        \Pr[F_i=f_i\vert Y=y] &=& \frac{c(f_i,y)}{\sum_{f'_i\in \{0,1\}}{c(f_i,y)}} \\
        \end{eqnarray*}
        First, to calculate the formula in the denominator (${\sum_{f'_i\in \{0,1\}}{c(f_i,y)}}$), we declare total\_features\_labels with util.Counter(). And to get a cost of feature and label set, we extract data from training data. When we use division, to avoid zero error occurs, we set all value as 1 that include keys of 0 and 1.\\ 
        Second, to calculate the formula in the numerator ($c(f_i,y)$), we declare conditional\_feature\_label with util.Counter(). In this case, we only need to add pixel,which called feature that value is greater than 1. 
        


\end{document}
